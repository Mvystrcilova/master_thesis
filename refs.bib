@book{knuth1979tex,
  title={TEX and METAFONT: New directions in typesetting},
  author={Knuth, Donald Ervin},
  year={1979},
  publisher={American Mathematical Society}
}

@book{lamport1994latex,
  title={LATEX: a document preparation system: user's guide and reference manual},
  author={Lamport, Leslie},
  year={1994},
  publisher={Addison-Wesley}
}

@book{glasman2010science,
  title={Science research writing for non-native speakers of English},
  author={Glasman-Deal, Hilary},
  year={2010},
  publisher={World Scientific}
}


@article{pearson-vii-1895,
	title = {{VII}. {Note} on regression and inheritance in the case of two parents},
	volume = {58},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspl.1895.0041},
	doi = {10.1098/rspl.1895.0041},
	abstract = {Consider a population in which sexual selection and natural selection may or may not be taking place. Assume only that the deviations from the mean in the case of any organ of any generation follow exactly or closely the normal law of frequency, then the following expressions may be shown to give the law of inheritance of the population.},
	number = {347-352},
	urldate = {2021-04-27},
	journal = {Proceedings of the Royal Society of London},
	author = {Pearson, Karl and Galton, Francis},
	month = jan,
	year = {1895},
	pages = {240--242},
}


@article{butterworth1930theory,
  title={On the theory of filter amplifiers},
  author={Butterworth, Stephen and others},
  journal={Wireless Engineer},
  volume={7},
  number={6},
  pages={536--541},
  year={1930}
}

@article{dilated-conv,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	urldate = {2021-04-28},
	journal = {arXiv:1603.07285 [cs, stat]},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = jan,
	year = {2018},
	note = {arXiv: 1603.07285},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}


@book{sparling1989english,
  title={English or Czenglish? Jak se vyhnout čechismům v angličtině},
  author={Sparling, Don},
  year={1989},
  publisher={Státní pedagogické nakladatelství}
}

@article{hammer-role-2013,
	title = {The role of {ECoG} magnitude and phase in decoding position, velocity, and acceleration during continuous motor behavior},
	volume = {7},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2013.00200/full},
	doi = {10.3389/fnins.2013.00200},
	abstract = {In neuronal population signals, including the electroencephalogram (EEG) and electrocorticogram (ECoG), the low-frequency component (LFC) is particularly informative about motor behavior and can be used for decoding movement parameters for brain-machine interface (BMI) applications. An idea previously expressed, but as of yet not quantitatively tested, is that it is the LFC phase that is the main source of decodable information. To test this issue, we analyzed human ECoG recorded during a game-like, one-dimensional, continuous motor task with a novel decoding method suitable for unfolding magnitude and phase explicitly into a complex-valued, time-frequency signal representation, enabling quantification of the decodable information within the temporal, spatial and frequency domains and allowing disambiguation of the phase contribution from that of the spectral magnitude. The decoding accuracy based only on phase information was substantially (at least 2 fold) and significantly higher than that based only on magnitudes for position, velocity and acceleration. The frequency profile of movement-related information in the ECoG data matched well with the frequency profile expected when assuming a close time-domain correlate of movement velocity in the ECoG, e.g., a (noisy) “copy” of hand velocity. No such match was observed with the frequency profiles expected when assuming a copy of either hand position or acceleration. There was also no indication of additional magnitude-based mechanisms encoding movement information in the LFC range. Thus, our study contributes to elucidating the nature of the informative low-frequency component of motor cortical population activity and may hence contribute to improve decoding strategies and BMI performance.},
	language = {English},
	urldate = {2021-04-24},
	journal = {Frontiers in Neuroscience},
	author = {Hammer, Jiří and Fischer, Jörg and Ruescher, Johanna and Schulze-Bonhage, Andreas and Aertsen, Ad and Ball, Tonio},
	year = {2013},
	keywords = {Decoding, Fourier de, brain-machine interfaces, low-frequency component, phase},
}

@article{kingma-adam-2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	urldate = {2021-04-25},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@article{clevert-elu-2016,
	title = {Fast and {Accurate} {Deep} {Network} {Learning} by {Exponential} {Linear} {Units} ({ELUs})},
	url = {http://arxiv.org/abs/1511.07289},
	urldate = {2021-04-24},
	journal = {arXiv:1511.07289 [cs]},
	author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.07289},
	keywords = {Computer Science - Machine Learning},
}


@article{eickhoff-new-2005,
	title = {A new {SPM} toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data},
	volume = {25},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S105381190400792X},
	doi = {10.1016/j.neuroimage.2004.12.034},
	language = {en},
	number = {4},
	urldate = {2021-04-24},
	journal = {NeuroImage},
	author = {Eickhoff, Simon B. and Stephan, Klaas E. and Mohlberg, Hartmut and Grefkes, Christian and Fink, Gereon R. and Amunts, Katrin and Zilles, Karl},
	month = may,
	year = {2005},
	pages = {1325--1335},
}

@unpublished{Hammer-2021,
author  = {Hammer, Jiří and Schirrmeister, Robin T. and Hartmann, Kay and Schulze-Bondage, Andreas and Ball, Tonio},
title   = {Functional Integration and Segregation Emerging in Deep Convolutional Networks Trained for Brain Signal Decoding},
year    = {2021},
note    = {unpublished},
}


@article{liu-effects-2015,
	title = {The effects of spatial filtering and artifacts on electrocorticographic signals},
	volume = {12},
	issn = {1741-2560, 1741-2552},
	url = {https://iopscience.iop.org/article/10.1088/1741-2560/12/5/056008},
	doi = {10.1088/1741-2560/12/5/056008},
	number = {5},
	urldate = {2021-04-08},
	journal = {Journal of Neural Engineering},
	author = {Liu, Y and Coon, W G and Pesters, A de and Brunner, P and Schalk, G},
	month = oct,
	year = {2015},
	pages = {056008},
}

@article{34-gunduz-hg,
author = {Aysegul Gunduz and Peter Brunner and Mohit Sharma and Eric C. Leuthardt and Anthony L. Ritaccio and Bijan Pesaran and Gerwin Schalk},
title = {Differential roles of high gamma and local motor potentials for movement preparation and execution},
journal = {Brain-Computer Interfaces},
volume = {3},
number = {2},
pages = {88-102},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/2326263X.2016.1179087},

URL = { 
        https://doi.org/10.1080/2326263X.2016.1179087
    
},
eprint = { 
        https://doi.org/10.1080/2326263X.2016.1179087
    
}}

@article{ica,
title = {Data-driven re-referencing of intracranial EEG based on independent component analysis (ICA)},
journal = {Journal of Neuroscience Methods},
volume = {307},
pages = {125-137},
year = {2018},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2018.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S0165027018301997},
author = {Sebastian Michelmann and Matthias S. Treder and Benjamin Griffiths and Casper Kerrén and Frédéric Roux and Maria Wimber and David Rollings and Vijay Sawlani and Ramesh Chelvarajah and Stephanie Gollwitzer and Gernot Kreiselmeyer and Hajo Hamer and Howard Bowman and Bernhard Staresina and Simon Hanslmayr},
keywords = {Independent component analysis, Intracranial EEG, Referencing, Preprocessing, Bipolar reference, Depth electrodes, Neuroscience, Electroencephalography},
}

@article{yao2019reference,
  title={Which Reference Should We Use for EEG and ERP practice?},
  author={Yao, Dezhong and Qin, Yun and Hu, Shiang and Dong, Li and Vega, Maria L Bringas and Sosa, Pedro A Vald{\'e}s},
  journal={Brain Topography},
  volume={32},
  number={4},
  pages={530},
  year={2019},
  publisher={Springer}
}

@article{lapique-1907,
author = {Brunel, Nicolas and van Rossum, Mark},
year = {2008},
month = {01},
pages = {337-9},
title = {Lapicque's 1907 paper: From frogs to integrate-and-fire},
volume = {97},
journal = {Biological cybernetics},
doi = {10.1007/s00422-007-0190-0}
}

@article{hodgkin1952quantitative,
  title={A quantitative description of membrane current and its application to conduction and excitation in nerve},
  author={Hodgkin, Alan L and Huxley, Andrew F},
  journal={The Journal of physiology},
  volume={117},
  number={4},
  pages={500--544},
  year={1952},
  publisher={Wiley Online Library}
}

@ARTICLE{ecog-bci,
  author={Schalk, Gerwin and Leuthardt, Eric C.},
  journal={IEEE Reviews in Biomedical Engineering}, 
  title={Brain-Computer Interfaces Using Electrocorticographic Signals}, 
  year={2011},
  volume={4},
  number={},
  pages={140-154},
  doi={10.1109/RBME.2011.2172408}}



@article{sleep-stage-alg-comparison,
  title={A comparative study on classification of sleep stage based on EEG signals using feature selection and classification algorithms},
  author={{\c{S}}en, Baha and Peker, Musa and {\c{C}}avu{\c{s}}o{\u{g}}lu, Abdullah and {\c{C}}elebi, Fatih V},
  journal={Journal of medical systems},
  volume={38},
  number={3},
  pages={1--21},
  year={2014},
  publisher={Springer}
}

@article{alzheimer-eeg,
  title={Fractality and a wavelet-chaos-methodology for EEG-based diagnosis of Alzheimer disease},
  author={Ahmadlou, Mehran and Adeli, Hojjat and Adeli, Anahita},
  journal={Alzheimer Disease \& Associated Disorders},
  volume={25},
  number={1},
  pages={85--92},
  year={2011},
  publisher={LWW}
}

@article{Zhang-2019,
	doi = {10.1088/1741-2552/ab3471},
	url = {https://doi.org/10.1088/1741-2552/ab3471},
	year = 2019,
	month = {10},
	publisher = {{IOP} Publishing},
	volume = {16},
	number = {6},
	pages = {066004},
	author = {Ruilong Zhang and Qun Zong and Liqian Dou and Xinyi Zhao},
	title = {A novel hybrid deep learning scheme for four-class motor imagery classification},
	journal = {Journal of Neural Engineering},
	}
	
@article{eeg-net,
	doi = {10.1088/1741-2552/aace8c},
	url = {https://doi.org/10.1088/1741-2552/aace8c},
	year = 2018,
	month = {7},
	publisher = {{IOP} Publishing},
	volume = {15},
	number = {5},
	pages = {056013},
	author = {Vernon J Lawhern and Amelia J Solon and Nicholas R Waytowich and Stephen M Gordon and Chou P Hung and Brent J Lance},
	title = {{EEGNet}: a compact convolutional neural network for {EEG}-based brain{\textendash}computer interfaces},
	journal = {Journal of Neural Engineering},
}
@article{sleep-eegnet,
author = {Mousavi, Sajad and Afghah, Fatemeh and Acharya, U Rajendra},
year = {2019},
month = {03},
pages = {},
title = {SleepEEGNet: Automated Sleep Stage Scoring with Sequence to Sequence Deep Learning Approach}
}

@article{bayesian-decoding,
author = {Wang, Z and Ji, Qiang and Miller, Kai and Schalk, Gerwin},
year = {2011},
month = {11},
pages = {127},
title = {Prior Knowledge Improves Decoding of Finger Flexion from Electrocorticographic Signals},
volume = {5},
journal = {Frontiers in neuroscience},
doi = {10.3389/fnins.2011.00127}
}

@article{conv-diagram,
author = {Yun-Cheng Tsai and Jun-Hao Chen and Jun-Jie Wang},
doi = {doi:10.1515/jisys-2018-0074},
url = {https://doi.org/10.1515/jisys-2018-0074},
title = {Predict Forex Trend via Convolutional Neural Networks},
journal = {Journal of Intelligent Systems},
number = {1},
volume = {29},
year = {2020},
pages = {941--958}
}

@INBOOK{back-prop,
  author={Rumelhart, David E. and McClelland, James L.},
  booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}, 
  title={Learning Internal Representations by Error Propagation}, 
  year={1987},
  volume={},
  number={},
  pages={318-362},
  doi={}}


@article{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{conv-intro,
  author    = {Keiron O'Shea and
               Ryan Nash},
  title     = {An Introduction to Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1511.08458},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08458},
  archivePrefix = {arXiv},
  eprint    = {1511.08458},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OSheaN15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reccurent-neural-networks,
  author    = {Zachary Chase Lipton},
  title     = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1506.00019},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.00019},
  archivePrefix = {arXiv},
  eprint    = {1506.00019},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Lipton15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cond-rf-finger-class,
  title={Asynchronous decoding of finger movements from ECoG signals using long-range dependencies conditional random fields},
  author={J. D. Saa and Adriana de Pesters and M. Çetin},
  year={2016}
}

@article{gradient-visualization,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year={2014},
  publisher={Iclr}
}

@article{maximizing-activation,
author = {Erhan, Dumitru and Bengio, Y. and Courville, Aaron and Vincent, Pascal},
year = {2009},
month = {01},
pages = {},
title = {Visualizing Higher-Layer Features of a Deep Network},
journal = {Technical Report, Univeristé de Montréal}
}

@article{class-activation-maps,
  author    = {Bolei Zhou and
               Aditya Khosla and
               {\`{A}}gata Lapedriza and
               Aude Oliva and
               Antonio Torralba},
  title     = {Learning Deep Features for Discriminative Localization},
  journal   = {CoRR},
  volume    = {abs/1512.04150},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.04150},
  archivePrefix = {arXiv},
  eprint    = {1512.04150},
  timestamp = {Mon, 13 Aug 2018 16:47:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhouKLOT15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lda-finger-movement-classification,
  title={Individual finger control of a modular prosthetic limb using high-density electrocorticography in a human subject},
  author={Hotson, Guy and McMullen, David P and Fifer, Matthew S and Johannes, Matthew S and Katyal, Kapil D and Para, Matthew P and Armiger, Robert and Anderson, William S and Thakor, Nitish V and Wester, Brock A and others},
  journal={Journal of neural engineering},
  volume={13},
  number={2},
  pages={026017},
  year={2016},
  publisher={IOP Publishing}
}

@article{lotte2018review,
  title={A review of classification algorithms for EEG-based brain--computer interfaces: a 10 year update},
  author={Lotte, Fabien and Bougrain, Laurent and Cichocki, Andrzej and Clerc, Maureen and Congedo, Marco and Rakotomamonjy, Alain and Yger, Florian},
  journal={Journal of neural engineering},
  volume={15},
  number={3},
  pages={031005},
  year={2018},
  publisher={IOP Publishing}
}

@article{markov-models-decoding,
  title={Multiscale semi-markov dynamics for intracortical brain-computer interfaces},
  author={Milstein, Daniel and Pacheco, Jason and Hochberg, Leigh and Simeral, John D and Jarosiewicz, Beata and Sudderth, Erik},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={868--878},
  year={2017}
  }
@INPROCEEDINGS{linear-regression-eeg-hand-3d,
  author={Ofner, Patrick and Müller-Putz, Gernot R.},
  booktitle={2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
  title={Decoding of velocities and positions of 3D arm movement from EEG}, 
  year={2012},
  volume={},
  number={},
  pages={6406-6409},
  doi={10.1109/EMBC.2012.6347460}}


@article{kalman-filters-velocity,
  title={Decoding hand movement velocity from electroencephalogram signals during a drawing task},
  author={Lv, Jun and Li, Yuanqing and Gu, Zhenghui},
  journal={Biomedical engineering online},
  volume={9},
  number={1},
  pages={1--21},
  year={2010},
  publisher={BioMed Central}
}

@inproceedings{uns-kalman-filters-gait-decoding,
author = {Luu, Trieu Phat and He, Yongtian and Nakagome, Sho and Gorges, Jeff and Nathan, Kevin and Contreras-Vidal, José},
year = {2016},
month = {08},
pages = {},
title = {Unscented Kalman Filter for Neural Decoding of Human Treadmill Walking from Non-invasive Electroencephalography},
volume = {2016},
journal = {Conference proceedings: ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Conference},
doi = {10.1109/EMBC.2016.7591006}
}

@article{eeg-hand-moving,
  title={Coarse Electrocorticographic Decoding of Ipsilateral Reach in Patients with Brain Lesions},
  author={G. Hotson and M. Fifer and S. Acharya and H. Benz and W. Anderson and N. Thakor and N. Crone},
  journal={PLoS ONE},
  year={2014},
  volume={9}
}

@article{gait-decoding,
author = {Nakagome, Sho and Luu, Trieu Phat and He, Yongtian and Sujatha Ravindran, Akshay and Contreras-Vidal, José},
year = {2020},
month = {03},
pages = {},
title = {An empirical comparison of neural networks and machine learning algorithms for EEG gait decoding},
volume = {10},
journal = {Scientific Reports},
doi = {10.1038/s41598-020-60932-4}
}

@article{fbcsp-bciv,
  title={Filter bank common spatial pattern algorithm on BCI competition IV datasets 2a and 2b},
  author={Ang, Kai Keng and Chin, Zheng Yang and Wang, Chuanchu and Guan, Cuntai and Zhang, Haihong},
  journal={Frontiers in neuroscience},
  volume={6},
  pages={39},
  year={2012},
  publisher={Frontiers}
}

@INPROCEEDINGS{lda-paper,
  author={Mika, S. and Ratsch, G. and Weston, J. and Scholkopf, B. and Mullers, K.R.},
  booktitle={Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468)}, 
  title={Fisher discriminant analysis with kernels}, 
  year={1999},
  volume={},
  number={},
  pages={41-48},
  doi={10.1109/NNSP.1999.788121}}

@article{qda-paper,
  title={Regularized discriminant analysis},
  author={Friedman, Jerome H},
  journal={Journal of the American statistical association},
  volume={84},
  number={405},
  pages={165--175},
  year={1989},
  publisher={Taylor \& Francis}
}

@ARTICLE{svm-alg,
  author={Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal={IEEE Intelligent Systems and their Applications}, 
  title={Support vector machines}, 
  year={1998},
  volume={13},
  number={4},
  pages={18-28},
  doi={10.1109/5254.708428}}


@article{Roy-2019,
	doi = {10.1088/1741-2552/ab260c},
	url = {https://doi.org/10.1088/1741-2552/ab260c},
	year = 2019,
	month = {8},
	publisher = {{IOP} Publishing},
	volume = {16},
	number = {5},
	pages = {051001},
	author = {Yannick Roy and Hubert Banville and Isabela Albuquerque and Alexandre Gramfort and Tiago H Falk and Jocelyn Faubert},
	title = {Deep learning-based electroencephalography analysis: a systematic review},
	journal = {Journal of Neural Engineering},
	}

@article{gamma-eeg-bad-resolution,
author = {Light, Gregory and Williams, Lisa and Minow, Falk and Sprock, Joyce and Rissling, Anthony and Sharp, Richard and Swerdlow, Neal and Braff, David},
year = {2010},
month = {07},
pages = {Unit 6.25.1-24},
title = {Electroencephalography (EEG) and Event-Related Potentials (ERPs) with Human Participants},
volume = {Chapter 6},
isbn = {0471142301},
journal = {Current protocols in neuroscience / editorial board, Jacqueline N. Crawley ... [et al.]},
doi = {10.1002/0471142301.ns0625s52}
}

@misc{dnn-nlp,
      title={Compressive Transformers for Long-Range Sequence Modelling}, 
      author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Timothy P. Lillicrap},
      year={2019},
      eprint={1911.05507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dnn-computer-vision,
      title={High-Performance Large-Scale Image Recognition Without Normalization}, 
      author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},
      year={2021},
      eprint={2102.06171},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{relu-paper,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Icml},
  year={2010}
}

@inproceedings{dense-prediction-images,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}
@article{drop-out,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@inproceedings{batch-norm,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{cnn-description,
title = {Recent advances in convolutional neural networks},
journal = {Pattern Recognition},
volume = {77},
pages = {354-377},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317304120},
author = {Jiuxiang Gu and Zhenhua Wang and Jason Kuen and Lianyang Ma and Amir Shahroudy and Bing Shuai and Ting Liu and Xingxing Wang and Gang Wang and Jianfei Cai and Tsuhan Chen},
keywords = {Convolutional neural network, Deep learning},
abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.}
}

@article{eeg-bci,
title = {An EEG-based brain-computer interface for cursor control},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {78},
number = {3},
pages = {252-259},
year = {1991},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(91)90040-B},
url = {https://www.sciencedirect.com/science/article/pii/001346949190040B},
author = {Jonathan R. Wolpaw and Dennis J. McFarland and Gregory W. Neat and Catherine A. Forneris},
keywords = {EEG, Prosthesis, Operant conditioning, Mu rhythm, Sensorimotor rhythm, Computer control, Communication},
}
@article{epileptic-seizures-eeg,
title = {Epileptic seizure detection in EEG signals using tunable-Q factor wavelet transform and bootstrap aggregating},
journal = {Computer Methods and Programs in Biomedicine},
volume = {137},
pages = {247-259},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716304370},
author = {Ahnaf Rashik Hassan and Siuly Siuly and Yanchun Zhang},
keywords = {EEG, Epilepsy seizure, Classification, TQWT, Bagging},
}

@ARTICLE{auto-regressive-eeg,
  
AUTHOR={Walter, Armin and Murguialday, Ander Ramos and Rosenstiel, Wolfgang and Birbaumer, Niels and Bogdan, Martin},   
	 
TITLE={Coupling BCI and cortical stimulation for brain-state-dependent stimulation: methods for spectral estimation in the presence of stimulation after-effects},      
	
JOURNAL={Frontiers in Neural Circuits},      
	
VOLUME={6},      

PAGES={87},     
	
YEAR={2012},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncir.2012.00087},       
	
DOI={10.3389/fncir.2012.00087},      
	
ISSN={1662-5110},   
   
}

@article{adaptive-laplacian-reference,
author = {Lu, Jun and Mcfarland, Dennis and Wolpaw, Jonathan},
year = {2012},
pages = {016002},
title = {Adaptive Laplacian filtering for sensorimotor rhythm-based brain-computer interfaces},
volume = {10},
journal = {Journal of neural engineering},
doi = {10.1088/1741-2560/10/1/016002}
}

@article{laplacian-reference,
title = {Optimal referencing for stereo-electroencephalographic (SEEG) recordings},
journal = {NeuroImage},
volume = {183},
pages = {327-335},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.08.020},
author = {Guangye Li and Shize Jiang and Sivylla E. Paraskevopoulou and Meng Wang and Yang Xu and Zehan Wu and Liang Chen and Dingguo Zhang and Gerwin Schalk},
keywords = {Stereo-electroencephalography, SEEG, Referencing method, Signal quality, Noise subtraction},
}

@article{36-vugt-2007,
author = {Van Vugt, Marieke and Sederberg, Per and Kahana, Michael},
year = {2007},
month = {01},
pages = {49-63},
title = {Comparison of spectral analysis methods for characterizing brain oscillations},
volume = {162},
journal = {Journal of Neuroscience Methods}
}

@article{ball-hg-importance,
author = {Ball, Tonio and Demandt, Évariste and Collins, Isabella and Kobak, Eva and Mehring, Carsten and Vogt, Klaus and Aertsen, Ad and Schulze-Bonhage, Andreas},
year = {2008},
month = {07},
pages = {302-10},
title = {Movement related activity in the high gamma range of the human EEG},
volume = {41},
journal = {NeuroImage},
doi = {10.1016/j.neuroimage.2008.02.032}
}

@article{schalk-2007,
author = {Schalk, Gerwin and Kubanek, Jan and Miller, Kai and Anderson, N.R. and Leuthardt, Eric and Ojemann, Jeffrey and Limbrick, D and Moran, DW and Gerhardt, Lester and Wolpaw, Jonathan},
year = {2007},
month = {10},
pages = {264-75},
title = {Decoding two-dimensional movement trajectories using electrocorticographic signals in humans},
volume = {4},
journal = {Journal of neural engineering},
doi = {10.1088/1741-2560/4/3/012}
}

@article{ball-2019,
author = {Ball, Tonio and Schulze-Bonhage, Andreas and Aertsen, Ad and Mehring, Carsten},
year = {2009},
month = {03},
pages = {016006},
title = {Differential representation of arm movement direction in relation to cortical anatomy and function},
volume = {6},
journal = {Journal of neural engineering},
doi = {10.1088/1741-2560/6/1/016006}
}

@article{Pistohl2008PredictionOA,
  title={Prediction of arm movement trajectories from ECoG-recordings in humans},
  author={T. Pistohl and T. Ball and A. Schulze-Bonhage and A. Aertsen and C. Mehring},
  journal={Journal of Neuroscience Methods},
  year={2008},
  volume={167},
  pages={105-114}
}

@article{multitaper-31,
author = {Pistohl, Tobias and Schulze-Bonhage, Andreas and Aertsen, Ad and Mehring, Carsten and Ball, Tonio},
year = {2011},
month = {07},
pages = {248-60},
title = {Decoding natural grasp types from human ECoG},
volume = {59},
journal = {NeuroImage},
doi = {10.1016/j.neuroimage.2011.06.084}
}

@article{anderson-offline-2009,
	title = {An Offline Evaluation of the Autoregressive Spectrum for Electrocorticography},
	volume = {56},
	issn = {0018-9294},
	url = {http://ieeexplore.ieee.org/document/4838922/},
	doi = {10.1109/TBME.2009.2009767},
	pages = {913--916},
	number = {3},
	journaltitle = {{IEEE} Transactions on Biomedical Engineering},
	shortjournal = {{IEEE} Trans. Biomed. Eng.},
	author = {Anderson, N.R. and Wisneski, K. and Eisenman, L. and Moran, D.W. and Leuthardt, E.C. and Krusienski, D.J.},
	urldate = {2021-04-29},
	date = {2009-03},
}

@book {NiedermeyersElectroencephalography,
      author = "Donald L. Schomer and Fernando H. Lopes da Silva",
      title = "Niedermeyer's ElectroencephalographyBasic Principles, Clinical Applications, and Related Fields: Basic Principles, Clinical Applications, and Related Fields",
      year = "2017",
      month = "11",
      publisher = "Oxford University Press",
      address = "Oxford, UK",
      isbn = "9780190228514",
      doi = "10.1093/med/9780190228484.001.0001",
      url = "https://oxfordmedicine.com/view/10.1093/med/9780190228484.001.0001/med-9780190228484"
}

@article{volkova-review,
author = {Volkova, Ksenia and Lebedev, Mikhail and Kaplan, Alexander and Ossadtchi, Alexei},
year = {2019},
month = {12},
pages = {74},
title = {Decoding Movement From Electrocorticographic Activity: A Review},
volume = {13},
journal = {Frontiers in Neuroinformatics},
doi = {10.3389/fninf.2019.00074}
}

@article{lebedev-cortical-2005,
	title = {Cortical {Ensemble} {Adaptation} to {Represent} {Velocity} of an {Artificial} {Actuator} {Controlled} by a {Brain}-{Machine} {Interface}},
	volume = {25},
	copyright = {Copyright © 2005 Society for Neuroscience 0270-6474/05/254681-13.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/25/19/4681},
	doi = {10.1523/JNEUROSCI.4088-04.2005},
	language = {en},
	number = {19},
	urldate = {2021-04-08},
	journal = {Journal of Neuroscience},
	author = {Lebedev, Mikhail A. and Carmena, Jose M. and O'Doherty, Joseph E. and Zacksenhouse, Miriam and Henriquez, Craig S. and Principe, Jose C. and Nicolelis, Miguel A. L.},
	month = may,
	year = {2005},
	pmid = {15888644},
	keywords = {body schema, brain-machine interface, cortical plasticity, macaque monkey, motor cortex, motor learning},
	pages = {4681--4693},
}

@misc{scholg-presence-2002,
	title = {Presence {Research} and {EEG}},
	language = {en},
	journal = {undefined},
	author = {Scholg, A. and Slater, M. and Pfurtscheller, G.},
	year = {2002},
	url = {https://www.semanticscholar.org/paper/Presence-Research-and-EEG-Scholg-Slater/18816255d88653d9bbaedb7a24c4394a6663c7a7}
}

@article{buzsaki-origin-2012,
	title = {The origin of extracellular fields and currents — {EEG}, {ECoG}, {LFP} and spikes},
	volume = {13},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/nrn3241},
	doi = {10.1038/nrn3241},
	language = {en},
	number = {6},
	urldate = {2021-04-07},
	journal = {Nature Reviews Neuroscience},
	author = {Buzsáki, György and Anastassiou, Costas A. and Koch, Christof},
	month = jun,
	year = {2012},
	pages = {407--420},
}

@article{tam-human-2019,
	title = {Human motor decoding from neural signals: a review},
	volume = {1},
	issn = {2524-4426},
	shorttitle = {Human motor decoding from neural signals},
	url = {https://doi.org/10.1186/s42490-019-0022-z},
	doi = {10.1186/s42490-019-0022-z},
	abstract = {Many people suffer from movement disability due to amputation or neurological diseases. Fortunately, with modern neurotechnology now it is possible to intercept motor control signals at various points along the neural transduction pathway and use that to drive external devices for communication or control. Here we will review the latest developments in human motor decoding. We reviewed the various strategies to decode motor intention from human and their respective advantages and challenges. Neural control signals can be intercepted at various points in the neural signal transduction pathway, including the brain (electroencephalography, electrocorticography, intracortical recordings), the nerves (peripheral nerve recordings) and the muscles (electromyography). We systematically discussed the sites of signal acquisition, available neural features, signal processing techniques and decoding algorithms in each of these potential interception points. Examples of applications and the current state-of-the-art performance were also reviewed. Although great strides have been made in human motor decoding, we are still far away from achieving naturalistic and dexterous control like our native limbs. Concerted efforts from material scientists, electrical engineers, and healthcare professionals are needed to further advance the field and make the technology widely available in clinical use.},
	number = {1},
	urldate = {2021-04-07},
	journal = {BMC Biomedical Engineering},
	author = {Tam, Wing-kin and Wu, Tong and Zhao, Qi and Keefer, Edward and Yang, Zhi},
	month = sep,
	year = {2019},
	keywords = {Brain-machine interfaces, Motor decoding, Neural signal processing, Neuroprosthesis},
	pages = {22},
}

@article{dense-prediction-machine-translation,
  author    = {Nal Kalchbrenner and
               Lasse Espeholt and
               Karen Simonyan and
               A{\"{a}}ron van den Oord and
               Alex Graves and
               Koray Kavukcuoglu},
  title     = {Neural Machine Translation in Linear Time},
  journal   = {CoRR},
  volume    = {abs/1610.10099},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.10099},
  archivePrefix = {arXiv},
  eprint    = {1610.10099},
  timestamp = {Mon, 13 Aug 2018 16:46:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KalchbrennerESO16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{hammer-predominance-2016,
	title = {Predominance of {Movement} {Speed} {Over} {Direction} in {Neuronal} {Population} {Signals} of {Motor} {Cortex}: {Intracranial} {EEG} {Data} and {A} {Simple} {Explanatory} {Model}},
	volume = {26},
	issn = {1047-3211, 1460-2199},
	shorttitle = {Predominance of {Movement} {Speed} {Over} {Direction} in {Neuronal} {Population} {Signals} of {Motor} {Cortex}},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhw033},
	doi = {10.1093/cercor/bhw033},
	language = {en},
	number = {6},
	urldate = {2021-01-21},
	journal = {Cerebral Cortex},
	author = {Hammer, Jiří and Pistohl, Tobias and Fischer, Jörg and Kršek, Pavel and Tomášek, Martin and Marusič, Petr and Schulze-Bonhage, Andreas and Aertsen, Ad and Ball, Tonio},
	month = jun,
	year = {2016},
	pages = {2863--2881},
}
@article{pca,
  title={LIII. On lines and planes of closest fit to systems of points in space},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={2},
  number={11},
  pages={559--572},
  year={1901},
  publisher={Taylor \& Francis}
}

@article{zeiler-visualizing-2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2020-11-27},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{emotion-eeg,
  title={Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks},
  author={Zheng, Wei-Long and Lu, Bao-Liang},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={7},
  number={3},
  pages={162--175},
  year={2015},
  publisher={IEEE}
}

@article{springenberg-striving-2015,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	urldate = {2020-11-27},
	journal = {arXiv:1412.6806 [cs]},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.6806},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{bach-pixel-wise-2015,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	language = {en},
	number = {7},
	urldate = {2020-11-24},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	editor = {Suarez, Oscar Deniz},
	month = jul,
	year = {2015},
	pages = {e0130140},
}

@book{samek-explainable-2019,
	title = {Explainable {AI}: interpreting, explaining and visualizing deep learning},
	isbn = {9783030289546 9783030289553},
	shorttitle = {Explainable {AI}},
	url = {https://doi.org/10.1007/978-3-030-28954-6},
	abstract = {The development of "intelligent" systems that can take decisions and perform autonomously might lead to faster and more consistent decisions. A limiting factor for a broader adoption of AI technology is the inherent risks that come with giving up human control and oversight to "intelligent" machines. Forsensitive tasks involving critical infrastructures and affecting human well-being or health, it is crucial to limit the possibility of improper, non-robust and unsafe decisions and actions. Before deploying an AI system, we see a strong need to validate its behavior, and thus establish guarantees that it will continue to perform as expected when deployed in a real-world environment. In pursuit of that objective, ways for humans to verify the agreement between the AI decision structure and their own ground-truth knowledge have been explored. Explainable AI (XAI) has developed as a subfield of AI, focused on exposing complex AI models to humans in a systematic and interpretable manner. The 22 chapters included in this book provide a timely snapshot of algorithms, theory, and applications of interpretable and explainable AI and AI techniques that have been proposed recently reflecting the current discourse in this field and providing directions of future development. The book is organized in six parts: towards AI transparency; methods for interpreting AI systems; explaining the decisions of AI systems; evaluating interpretability and explanations; applications of explainable AI; and software for explainable AI. --},
	language = {English},
	urldate = {2020-11-16},
	author = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	note = {OCLC: 1120722055},
}

@article{eitel-uncovering-2019,
	title = {Uncovering convolutional neural network decisions for diagnosing multiple sclerosis on conventional {MRI} using layer-wise relevance propagation},
	volume = {24},
	issn = {22131582},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2213158219303535},
	doi = {10.1016/j.nicl.2019.102003},
	language = {en},
	urldate = {2020-11-23},
	journal = {NeuroImage: Clinical},
	author = {Eitel, Fabian and Soehler, Emily and Bellmann-Strobl, Judith and Brandt, Alexander U. and Ruprecht, Klemens and Giess, René M. and Kuchling, Joseph and Asseyer, Susanna and Weygandt, Martin and Haynes, John-Dylan and Scheel, Michael and Paul, Friedemann and Ritter, Kerstin},
	year = {2019},
	pages = {102003},
}

@article{sundararajan-axiomatic-2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	urldate = {2020-11-23},
	journal = {arXiv:1703.01365 [cs]},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.01365},
	keywords = {Computer Science - Machine Learning},
}

@article{kindermans-learning-2017,
	title = {Learning how to explain neural networks: {PatternNet} and {PatternAttribution}},
	shorttitle = {Learning how to explain neural networks},
	url = {http://arxiv.org/abs/1705.05598},
	abstract = {DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.},
	urldate = {2020-11-20},
	journal = {arXiv:1705.05598 [cs, stat]},
	author = {Kindermans, Pieter-Jan and Schütt, Kristof T. and Alber, Maximilian and Müller, Klaus-Robert and Erhan, Dumitru and Kim, Been and Dähne, Sven},
	month = oct,
	year = {2017},
	note = {arXiv: 1705.05598},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fong-interpretable-2017,
	title = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	url = {http://arxiv.org/abs/1704.03296},
	doi = {10.1109/ICCV.2017.371},
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
	urldate = {2020-11-20},
	journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
	author = {Fong, Ruth and Vedaldi, Andrea},
	month = oct,
	year = {2017},
	note = {arXiv: 1704.03296},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3449--3457},
}

@article{guidotti-survey-2019,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3236009},
	doi = {10.1145/3236009},
	language = {en},
	number = {5},
	urldate = {2020-11-19},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = jan,
	year = {2019},
	pages = {1--42},
}

@article{yang-visual-2018,
	title = {Visual {Explanations} {From} {Deep} {3D} {Convolutional} {Neural} {Networks} for {Alzheimer}'s {Disease} {Classification}},
	url = {http://arxiv.org/abs/1803.02544},
	abstract = {We develop three efficient approaches for generating visual explanations from 3D convolutional neural networks (3D-CNNs) for Alzheimer's disease classification. One approach conducts sensitivity analysis on hierarchical 3D image segmentation, and the other two visualize network activations on a spatial map. Visual checks and a quantitative localization benchmark indicate that all approaches identify important brain parts for Alzheimer's disease diagnosis. Comparative analysis show that the sensitivity analysis based approach has difficulty handling loosely distributed cerebral cortex, and approaches based on visualization of activations are constrained by the resolution of the convolutional layer. The complementarity of these methods improves the understanding of 3D-CNNs in Alzheimer's disease classification from different perspectives.},
	urldate = {2020-11-19},
	journal = {arXiv:1803.02544 [cs, stat]},
	author = {Yang, Chengliang and Rangarajan, Anand and Ranka, Sanjay},
	month = jul,
	year = {2018},
	note = {arXiv: 1803.02544},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rieke-visualizing-2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visualizing {Convolutional} {Networks} for {MRI}-{Based} {Diagnosis} of {Alzheimer}’s {Disease}},
	isbn = {9783030026288},
	doi = {10.1007/978-3-030-02628-8_3},
	abstract = {Visualizing and interpreting convolutional neural networks (CNNs) is an important task to increase trust in automatic medical decision making systems. In this study, we train a 3D CNN to detect Alzheimer’s disease based on structural MRI scans of the brain. Then, we apply four different gradient-based and occlusion-based visualization methods that explain the network’s classification decisions by highlighting relevant areas in the input image. We compare the methods qualitatively and quantitatively. We find that all four methods focus on brain regions known to be involved in Alzheimer’s disease, such as inferior and middle temporal gyrus. While the occlusion-based methods focus more on specific regions, the gradient-based methods pick up distributed relevance patterns. Additionally, we find that the distribution of relevance varies across patients, with some having a stronger focus on the temporal lobe, whereas for others more cortical areas are relevant. In summary, we show that applying different visualization methods is important to understand the decisions of a CNN, a step that is crucial to increase clinical impact and trust in computer-based decision support systems.},
	language = {en},
	booktitle = {Understanding and {Interpreting} {Machine} {Learning} in {Medical} {Image} {Computing} {Applications}},
	publisher = {Springer International Publishing},
	author = {Rieke, Johannes and Eitel, Fabian and Weygandt, Martin and Haynes, John-Dylan and Ritter, Kerstin},
	editor = {Stoyanov, Danail and Taylor, Zeike and Kia, Seyed Mostafa and Oguz, Ipek and Reyes, Mauricio and Martel, Anne and Maier-Hein, Lena and Marquand, Andre F. and Duchesnay, Edouard and Löfstedt, Tommy and Landman, Bennett and Cardoso, M. Jorge and Silva, Carlos A. and Pereira, Sergio and Meier, Raphael},
	year = {2018},
	keywords = {3D , Alzheimer , Brain , CNN , Deep learning , MRI , Visualization },
	pages = {24--31},
}

@ARTICLE{autoencoder,
  author={Wen, Tingxi and Zhang, Zhongnan},
  journal={IEEE Access}, 
  title={Deep Convolution Neural Network and Autoencoders-Based Unsupervised Feature Learning of EEG Signals}, 
  year={2018},
  volume={6},
  number={},
  pages={25399-25410},
  doi={10.1109/ACCESS.2018.2833746}}


@article{feet-movement,
  title={Distinct cortical areas for motor preparation and execution in human identified by Bereitschaftspotential recording and ECoG-EMG coherence analysis},
  author={Satow, Takeshi and Matsuhashi, Masao and Ikeda, Akio and Yamamoto, Junichi and Takayama, Motohiro and Begum, Tahamina and Mima, Tatsuya and Nagamine, Takashi and Mikuni, Nobuhiro and Miyamoto, Susumu and others},
  journal={Clinical neurophysiology},
  volume={114},
  number={7},
  pages={1259--1264},
  year={2003},
  publisher={Elsevier}
}
@ARTICLE{wrist-flexion,
  author={Edelman, Bradley J. and Baxter, Bryan and He, Bin},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={EEG Source Imaging Enhances the Decoding of Complex Right-Hand Motor Imagery Tasks}, 
  year={2016},
  volume={63},
  number={1},
  pages={4-14},
  doi={10.1109/TBME.2015.2467312}}

@article{zintgraf-visualizing-2017,
	title = {Visualizing {Deep} {Neural} {Network} {Decisions}: {Prediction} {Difference} {Analysis}},
	shorttitle = {Visualizing {Deep} {Neural} {Network} {Decisions}},
	url = {http://arxiv.org/abs/1702.04595},
	abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
	urldate = {2020-11-17},
	journal = {arXiv:1702.04595 [cs]},
	author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04595},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{coherence-based,
author = {Delgado Saa, Jaime},
year = {2020},
month = {05},
pages = {},
title = {Using Coherence-based spectro-spatial filters for stimulus features prediction from electro-corticographic recordings},
journal = {Scientific Reports},
doi = {10.1038/s41598-020-63303-1}
}

@article{sixt-when-2020,
	title = {When {Explanations} {Lie}: {Why} {Many} {Modified} {BP} {Attributions} {Fail}},
	shorttitle = {When {Explanations} {Lie}},
	url = {http://arxiv.org/abs/1912.09818},
	abstract = {Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically. For code see: https://github.com/berleon/when-explanations-lie},
	urldate = {2020-11-17},
	journal = {arXiv:1912.09818 [cs, stat]},
	author = {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
	month = aug,
	year = {2020},
	note = {arXiv: 1912.09818},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ancona-towards-2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.06104},
	abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
	urldate = {2020-11-16},
	journal = {arXiv:1711.06104 [cs, stat]},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.06104},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rafegas-understanding-2020,
	title = {Understanding trained {CNNs} by indexing neuron selectivity},
	volume = {136},
	issn = {01678655},
	url = {http://arxiv.org/abs/1702.00382},
	doi = {10.1016/j.patrec.2019.10.013},
	abstract = {The impressive performance of Convolutional Neural Networks (CNNs) when solving different vision problems is shadowed by their black-box nature and our consequent lack of understanding of the representations they build and how these representations are organized. To help understanding these issues, we propose to describe the activity of individual neurons by their Neuron Feature visualization and quantify their inherent selectivity with two specific properties. We explore selectivity indexes for: an image feature (color); and an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer Conv4 or class selective neurons such as dog-face neurons in layer Conv5 in VGG-M, and establishes a methodology to derive other selectivity properties. Indexing on neuron selectivity can statistically draw how features and classes are represented through layers in a moment when the size of trained nets is growing and automatic tools to index neurons can be helpful.},
	urldate = {2020-11-17},
	journal = {Pattern Recognition Letters},
	author = {Rafegas, Ivet and Vanrell, Maria and Alexandre, Luis A. and Arias, Guillem},
	month = aug,
	year = {2020},
	note = {arXiv: 1702.00382},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {318--325},
}

@inproceedings{goodfellow-towards-2018,
	title = {Towards {Understanding} {ECG} {Rhythm} {Classification} {Using} {Convolutional} {Neural} {Networks} and {Attention} {Mappings}},
	abstract = {In this study, a deep convolutional neural network was trained to classify single lead ECG waveforms as either Normal Sinus Rhythm, Atrial Fibrillation, or Other Rhythm. The dataset consisted of 12,186 labeled waveforms donated by AliveCor R © for use in the 2017 Physionet Challenge. The study was run in two phases, the first to generate a classifier that performed at a level comparable to the top submission of the 2017 Physionet Challenge, and the second to extract class activation mappings to help better understand which areas of the waveform the model was focusing on when making a classification. The convolutional neural network had 13 layers, including dilated convolutions, max pooling, ReLU activation, batch normalization, and dropout. Class activation maps were generated using a global average pooling layer before the softmax layer. The model generated the following average scores, across all rhythm classes, on the validation dataset: precision=0.84, recall=0.85, F1=0.84, and accuracy=0.88. For the Normal Sinus Rhythm class activation maps, we observed roughly constant attention, while for the Other Rhythm class, we observed attention spikes associated with premature beats. The class activation maps would allow for some level of interpretability by clinicians, which will likely be important for the adoption of these techniques to augment diagnosis. c © 2018 S.D. Goodfellow, A. Goodwin, R. Greer, P.C. Laussen, M. Mazwi \& D. Eytan. ECG Classification Using Convolutional Neural Networks and Attention Mappings},
	booktitle = {{MLHC}},
	author = {Goodfellow, S. and Goodwin, A. and Greer, Robert and Laussen, P. and Mazwi, Mjaye and Eytan, D.},
	year = {2018},
}

@article{angrick-speech-2018,
	title = {Speech {Synthesis} from {ECoG} using {Densely} {Connected} {3D} {Convolutional} {Neural} {Networks}},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/478644v1},
	doi = {10.1101/478644},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater} {\textless}p{\textgreater}Direct synthesis of speech from neural signals could provide a fast and natural way of communication to people with neurological diseases. Invasively-measured brain activity (electrocorticography; ECoG) supplies the necessary temporal and spatial resolution to decode fast and complex processes such as speech production. A number of impressive advances in speech decoding using neural signals have been achieved in recent years, but the complex dynamics are still not fully understood. However, it is unlikely that simple linear models can capture the relation between neural activity and continuous spoken speech.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Approach{\textless}/h3{\textgreater} {\textless}p{\textgreater}Here we show that deep neural networks can be used to map ECoG from speech production areas onto an intermediate representation of speech (logMel spectrogram). The proposed method uses a densely connected convolutional neural network topology which is well-suited to work with the small amount of data available from each participant.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Main results{\textless}/h3{\textgreater} {\textless}p{\textgreater}In a study with six participants, we achieved correlations up to \textit{r} = 0.69 between the reconstructed and original logMel spectrograms. We transfered our prediction back into an audible waveform by applying a Wavenet vocoder. The vocoder was conditioned on logMel features that harnessed a much larger, pre-existing data corpus to provide the most natural acoustic output.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Significance{\textless}/h3{\textgreater} {\textless}p{\textgreater}To the best of our knowledge, this is the first time that high-quality speech has been reconstructed from neural recordings during speech production using deep neural networks.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-11-16},
	journal = {bioRxiv},
	author = {Angrick, Miguel and Herff, Christian and Mugler, Emily and Tate, Matthew C. and Slutzky, Marc W. and Krusienski, Dean J. and Schultz, Tanja},
	month = nov,
	year = {2018},
	pages = {478644},
}

@article{montavon-methods-2018,
	title = {Methods for interpreting and understanding deep neural networks},
	volume = {73},
	issn = {10512004},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1051200417302385},
	doi = {10.1016/j.dsp.2017.10.011},
	language = {en},
	urldate = {2020-11-16},
	journal = {Digital Signal Processing},
	author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = feb,
	year = {2018},
	pages = {1--15},
}

@article{shrikumar-learning-2019,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
	urldate = {2020-11-16},
	journal = {arXiv:1704.02685 [cs]},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = oct,
	year = {2019},
	note = {arXiv: 1704.02685},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{zhou-learning-2016,
	address = {Las Vegas, NV, USA},
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	isbn = {9781467388511},
	url = {http://ieeexplore.ieee.org/document/7780688/},
	doi = {10.1109/CVPR.2016.319},
	urldate = {2020-11-16},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	month = jun,
	year = {2016},
	pages = {2921--2929},
}

@article{sturm-interpretable-2016,
	title = {Interpretable {Deep} {Neural} {Networks} for {Single}-{Trial} {EEG} {Classification}},
	url = {http://arxiv.org/abs/1604.08201},
	abstract = {Background: In cognitive neuroscience the potential of Deep Neural Networks (DNNs) for solving complex classification tasks is yet to be fully exploited. The most limiting factor is that DNNs as notorious 'black boxes' do not provide insight into neurophysiological phenomena underlying a decision. Layer-wise Relevance Propagation (LRP) has been introduced as a novel method to explain individual network decisions. New Method: We propose the application of DNNs with LRP for the first time for EEG data analysis. Through LRP the single-trial DNN decisions are transformed into heatmaps indicating each data point's relevance for the outcome of the decision. Results: DNN achieves classification accuracies comparable to those of CSP-LDA. In subjects with low performance subject-to-subject transfer of trained DNNs can improve the results. The single-trial LRP heatmaps reveal neurophysiologically plausible patterns, resembling CSP-derived scalp maps. Critically, while CSP patterns represent class-wise aggregated information, LRP heatmaps pinpoint neural patterns to single time points in single trials. Comparison with Existing Method(s): We compare the classification performance of DNNs to that of linear CSP-LDA on two data sets related to motor-imaginery BCI. Conclusion: We have demonstrated that DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of high-resolution assessment of neural activity can be reached. LRP is a potential remedy for the lack of interpretability of DNNs that has limited their utility in neuroscientific applications. The extreme specificity of the LRP-derived heatmaps opens up new avenues for investigating neural activity underlying complex perception or decision-related processes.},
	urldate = {2020-11-16},
	journal = {arXiv:1604.08201 [cs, stat]},
	author = {Sturm, Irene and Bach, Sebastian and Samek, Wojciech and Müller, Klaus-Robert},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08201},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lawhern-eegnet-2018,
	title = {{EEGNet}: a compact convolutional neural network for {EEG}-based brain–computer interfaces},
	volume = {15},
	issn = {1741-2560, 1741-2552},
	shorttitle = {{EEGNet}},
	url = {https://iopscience.iop.org/article/10.1088/1741-2552/aace8c},
	doi = {10.1088/1741-2552/aace8c},
	number = {5},
	urldate = {2020-11-16},
	journal = {Journal of Neural Engineering},
	author = {Lawhern, Vernon J and Solon, Amelia J and Waytowich, Nicholas R and Gordon, Stephen M and Hung, Chou P and Lance, Brent J},
	month = oct,
	year = {2018},
	pages = {056013},
}

@article{meisel-identifying-2019,
	title = {Identifying signal-dependent information about the preictal state: {A} comparison across {ECoG}, {EEG} and {EKG} using deep learning},
	volume = {45},
	issn = {23523964},
	shorttitle = {Identifying signal-dependent information about the preictal state},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352396419304360},
	doi = {10.1016/j.ebiom.2019.07.001},
	language = {en},
	urldate = {2020-11-16},
	journal = {EBioMedicine},
	author = {Meisel, Christian and Bailey, Kimberlyn A.},
	month = jul,
	year = {2019},
	pages = {422--431},
}

@article{hartmann-hierarchical-2018,
	title = {Hierarchical internal representation of spectral features in deep convolutional networks trained for {EEG} decoding},
	url = {http://arxiv.org/abs/1711.07792},
	doi = {10.1109/IWW-BCI.2018.8311493},
	abstract = {Recently, there is increasing interest and research on the interpretability of machine learning models, for example how they transform and internally represent EEG signals in Brain-Computer Interface (BCI) applications. This can help to understand the limits of the model and how it may be improved, in addition to possibly provide insight about the data itself. Schirrmeister et al. (2017) have recently reported promising results for EEG decoding with deep convolutional neural networks (ConvNets) trained in an end-to-end manner and, with a causal visualization approach, showed that they learn to use spectral amplitude changes in the input. In this study, we investigate how ConvNets represent spectral features through the sequence of intermediate stages of the network. We show higher sensitivity to EEG phase features at earlier stages and higher sensitivity to EEG amplitude features at later stages. Intriguingly, we observed a specialization of individual stages of the network to the classical EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first evidence that particularly in the last convolutional layer, the network learns to detect more complex oscillatory patterns beyond spectral phase and amplitude, reminiscent of the representation of complex visual features in later layers of ConvNets in computer vision tasks. Our findings thus provide insights into how ConvNets hierarchically represent spectral EEG features in their intermediate layers and suggest that ConvNets can exploit and might help to better understand the compositional structure of EEG time series.},
	urldate = {2020-11-16},
	journal = {2018 6th International Conference on Brain-Computer Interface (BCI)},
	author = {Hartmann, Kay Gregor and Schirrmeister, Robin Tibor and Ball, Tonio},
	month = jan,
	year = {2018},
	note = {arXiv: 1711.07792},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	pages = {1--6},
}

@techreport{petrosuan-decoding-2020,
	type = {preprint},
	title = {Decoding neural signals and discovering their representations with a compact and interpretable convolutional neural network},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.02.129114},
	abstract = {A
            bstract

          Brain-computer interfaces (BCIs) decode information from neural activity and send it to external devices. In recent years, we have seen an emergence of new algorithms for BCI decoding including those based on the deep-learning principles. Here we describe a compact convolutional network-based architecture for adaptive decoding of electrocorticographic (ECoG) data into finger kinematics. We also propose a theoretically justified approach to interpreting the spatial and temporal weights in the architectures that combine adaptation in both space and time, such as the one described here. In these architectures the weights are optimized not only to align with the target sources but also to tune away from the interfering ones, in both the spatial and the frequency domains. The obtained spatial and frequency patterns characterizing the neuronal populations pivotal to the specific decoding task can then be interpreted by fitting appropriate spatial and dynamical models.
          We first tested our solution using realistic Monte-Carlo simulations. Then, when applied to the ECoG data from Berlin BCI IV competition dataset, our architecture performed comparably to the competition winners without requiring explicit feature engineering. Moreover, using the proposed approach to the network weights interpretation we could unravel the spatial and the spectral patterns of the neuronal processes underlying the successful decoding of finger kinematics from another ECoG dataset with known sensor positions.
          As such, the proposed solution offers a good decoder and a tool for investigating neural mechanisms of motor control.},
	language = {en},
	urldate = {2020-11-16},
	institution = {Bioengineering},
	author = {Petrosuan, Arthur and Lebedev, Mikhail and Ossadtchi, Alexei},
	month = jun,
	year = {2020},
	doi = {10.1101/2020.06.02.129114},
}

@article{xie-cnn-lstm-finger-movement,
author = {Xie, Ziqian and Schwartz, Odelia and Prasad, Abhishek},
year = {2017},
month = {11},
pages = {},
title = {Decoding of finger trajectory from ECoG using Deep Learning},
volume = {15},
journal = {Journal of Neural Engineering},
doi = {10.1088/1741-2552/aa9dbe}
}

@article{cecotti-convolutional-2011,
	title = {Convolutional {Neural} {Networks} for {P300} {Detection} with {Application} to {Brain}-{Computer} {Interfaces}},
	volume = {33},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/5492691/},
	doi = {10.1109/TPAMI.2010.125},
	number = {3},
	urldate = {2020-11-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cecotti, H and Graser, A},
	month = mar,
	year = {2011},
	pages = {433--445},
}

@article{schirrmeister-deep-2017,
	title = {Deep learning with convolutional neural networks for {EEG} decoding and visualization: {Convolutional} {Neural} {Networks} in {EEG} {Analysis}},
	volume = {38},
	issn = {10659471},
	shorttitle = {Deep learning with convolutional neural networks for {EEG} decoding and visualization},
	url = {http://doi.wiley.com/10.1002/hbm.23730},
	doi = {10.1002/hbm.23730},
	language = {en},
	number = {11},
	urldate = {2020-11-16},
	journal = {Human Brain Mapping},
	author = {Schirrmeister, Robin Tibor and Springenberg, Jost Tobias and Fiederer, Lukas Dominique Josef and Glasstetter, Martin and Eggensperger, Katharina and Tangermann, Michael and Hutter, Frank and Burgard, Wolfram and Ball, Tonio},
	month = nov,
	year = {2017},
	pages = {5391--5420},
}

@article{wang-ajile-2018,
	title = {{AJILE} {Movement} {Prediction}: {Multimodal} {Deep} {Learning} for {Natural} {Human} {Neural} {Recordings} and {Video}},
	shorttitle = {{AJILE} {Movement} {Prediction}},
	url = {http://arxiv.org/abs/1709.05939},
	abstract = {Developing useful interfaces between brains and machines is a grand challenge of neuroengineering. An effective interface has the capacity to not only interpret neural signals, but predict the intentions of the human to perform an action in the near future; prediction is made even more challenging outside well-controlled laboratory experiments. This paper describes our approach to detect and to predict natural human arm movements in the future, a key challenge in brain computer interfacing that has never before been attempted. We introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset; AJILE includes automatically annotated poses of 7 upper body joints for four human subjects over 670 total hours (more than 72 million frames), along with the corresponding simultaneously acquired intracranial neural recordings. The size and scope of AJILE greatly exceeds all previous datasets with movements and electrocorticography (ECoG), making it possible to take a deep learning approach to movement prediction. We propose a multimodal model that combines deep convolutional neural networks (CNN) with long short-term memory (LSTM) blocks, leveraging both ECoG and video modalities. We demonstrate that our models are able to detect movements and predict future movements up to 800 msec before movement initiation. Further, our multimodal movement prediction models exhibit resilience to simulated ablation of input neural signals. We believe a multimodal approach to natural neural decoding that takes context into account is critical in advancing bioelectronic technologies and human neuroscience.},
	urldate = {2020-11-16},
	journal = {arXiv:1709.05939 [cs, q-bio]},
	author = {Wang, Nancy Xin Ru and Farhadi, Ali and Rao, Rajesh and Brunton, Bingni},
	month = mar,
	year = {2018},
	note = {arXiv: 1709.05939},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
}

@article{lstm-paper,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@article{brunner2008bci,
  title={BCI Competition 2008--Graz data set A},
  author={Brunner, Clemens and Leeb, Robert and M{\"u}ller-Putz, Gernot and Schl{\"o}gl, Alois and Pfurtscheller, GJIFKD},
  journal={Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz University of Technology},
  volume={16},
  pages={1--6},
  year={2008}
}