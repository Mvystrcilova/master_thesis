\chapter{Experiments and results}
\label{ch:exp}

\section{Gradient peak}\label{sec:gradient-peak}
As stated before, the input perturbation, output-correlation method in \cite{Hammer_2021} did not point to high-gamma frequency amplitude as being strikingly important to the network. The gradient visualization also did not show particularly high gradients in the high-gamma band, unless the input window for the network was shortened so that the network had only one output, i.e. predicted one time-point. Two interesting observations could be made for the one-output gradient visualization:
begin{itemize}
\item[1.] A gradient peak occurs at 83.88 Hz in both the untrained and the trained network and it is amplified with training. Figure \ref{}.
\item[2.] If we add noise of a certain frequency or white noise to the input, frequencies around 83.33 Hz increases in the output of the network. Figure \ref{}
end{itemize}

83.33Hz seems like a random frequency to have such a sharp peak in the frequency and also does not seem to be caused by a physiological property of the input signal.
A hypothesis was that the 83.33 Hz peak occurs due to this frequency being aligned with the dilations of the max-pool layers and therefore is just an architecture artifact.
Because sampling rate is 250 and the dilations of the max-pool layers are powers of three and 250/3 = 83.33Hz, this frequency aligns with the dilations of the max-pool layers which amplify it.
However, this does not explain why the peak at 83.33Hz is amplified with training as is apparent from Figure \ref{figure}.

To test the artifact hypothesis, we systematically changed the dilations of the max-pool layers to powers of 1, 2 and 3 and kernel sizes to 1\footnote{Importantly, when all max-pool layers in a network have their kernel size set to 1, it is equivalent to a network without max-pool layers.}, 2, 3 and 4

The results in Figure~\ref{Figure} demonstrate that the kernel size change has no effect on the 83.33Hz peak.
However, with change in the dilations, the 83.33 Hz peak disappears without a decrease in performance.
This suggests that the 83.33Hz peak really is an architecture artifact and it does not mean, the network is actually focusing on high-gamma and the 83.33 Hz frequency in particular.

We also studied how the signal is affected by max-pool layers only.
We used one and three max-pool layers with dilations equivalent with those from the Deep4Net.
When the max-pool layer dilation parameter was three (or powers of three in the case of multiple consecutive max-pool layers), the output signal exhibited periodical peaks with the greatest peak around 83.33Hz.
When the dilation of the max-pool layers shifted to two and powers of two, the periodicity of the peaks still occurred, however, the period increased and the greatest peak was around 120Hz.
This is in line with the hypothesis that the alignment of frequencies with the max-pool layer dilations is indeed the reason for the gradient peak we see in Figure~\ref{}.

\section{Architectural modifications}\label{sec:architectural-modifications}
\subsection{Performance}\label{subsec:performance}
While the gradient peak seemed like a dead end, while studying it, we noticed that some of the networks, especially those with smaller kernel sizes and/or dilation parameters in their max-pool layers seem to perform significantly better.
Therefore, we decided to do a thorough inspection of how each of the networks performs on the filtered datasets as described in Section~\ref{Filtering}.
The results can be seen in Figure~\cite{Figure} for velocity and \cite{Figure} for absolute velocity.
In the following points we summarize the findings on the different datasets.

\begin{itemize}
    \item \textbf{Full training and validation} Some of the networks significantly outperformed the Deep4Net sbp0 when both trained and validated on full data.The best performing network was the network where the max-pool layer had no influence, namely the one with max-pool layer kernel size 1.
    \item \textbf{Full training and low-pass validation} When the networks were trained on the full dataset and validated on the low-passed dataset (< 40Hz), the performance changed significantly for all the networks as is obvious from the Figure~\ref{}.
    Nevertheless, in order to achieve a statistically significant decrease in performance, we had to use a Butterworth filter of order 15 instead of order 3 which was previously used in \cite{Hammer\_2021}.The 3rd order filter caused no apparent change in performances.
    The Butterworth filter gradually attenuates the frequencies above the cut-off frequency (40Hz in this case).
    The higher the filter order, the steeper the attenuation is.
    Therefore, the fact that the performance decrease followed only after the stronger filter was employed suggests, that the network indeed focuses on frequencies above 40Hz but not particularly high frequencies (those in the high-gamma band) which were attenuated by both the 3rd as well as 15th order filter Figure~\ref{filters}.
    \item \textbf{High-pass training and validation} To see if the networks are able to use information from the high-gamma frequency band, the networks were trained and evaluated on the high-pass dataset (>60Hz).
    As is clear from Figure~\ref{figure\_hp\_performance}, the networks are able to use some information from the high-gamma especially when decoding absolute velocity where the correlations are often not only significantly above chance decoding but also achieve fairly good correlation coefficients.
    \item \textbf{Full training and high-pass validation} Another possibility to see if the networks when trained on full data are learning from the high-gamma frequency band, was to train the network on the full dataset and validate only on the high-passed data.
    As is obvious from Figure~\ref{Figure}, only few networks perform significantly better than chance level decoding. The conclusion that the network, when given access to, utilizes primarily information from the low end of the frequency spectrum can be drawn from this result.
    \item \textbf{Low-pass training and high-pass validation}
Training on low-passed data and validation on high-passed data was important to further study how the network operates.
We wanted to find out if it is able to transfer information between two completely separate datasets.
Because the cut-off frequency for the low-passed data is 40Hz and for the high-passed data 60Hz with a very steep filter, there is no frequency overlap between the two sets.
Therefore, it would be interesting but also rather surprising if from modulation in the low frequencies (below 40Hz) the network would learn to use information about modulations in the high frequencies (above 60Hz).
Nevertheless as obvious from Figure~\ref{fig}, the networks were unable to transfer any information.
\end{itemize}
The findings presented in this section lead to two interesting questions.
The first one  is what do the gradients of the various architectures look like and do some of them use high-gamma?
This analysis is described in Section\ref{subsec:gradients}.
The second question that arises is what if  we shift the predicted time-point to the centre of the receptive field.
This analysis is described in Section ref{shifting\_analysis}.

\subsection{Gradients}\label{subsec:gradients}
The differences in performance among the networks, reinforce the interest in the gradients of the various architectures.
Since some of the networks perform significantly better compared to the initial Deep4Net, we analyze gradients of the different architectures to see if the reason for a better performance is their ability to use information from the high-gamma band.
We perform the gradient visualization of all the architectures with kernel sizes 1, 2 and 3 and dilations as powers of 1, 2 and 3.

The results show differences in gradients among the architectures.
Gradients of all intermediate layers and the visualizations can be found in the Appendix.
Figure\ref{figure} includes the gradients of the output layer which in our opinion best represents the gradients of the other layers for all the architectures.

Based on the performances of the networks and the gradients these important and interesting observations can be made:

\begin{itemize}
    \item The networks focus on motor-channels when making predictions.
This is to be expected when they are tasked with decoding movement.
    \item There are obvious differences between the gradients for velocity and absolute velocity.
    Nevertheless, for both variables the network without max-pool, here denoted as {variable}_k1 is the best performing architecture.
    And it is in both cases also the network which is most interested in modulations in the low frequency bands.
    This suggests that using the information in the high-gamma frequency band is not necessarily an asset.
    \item The networks which exhibit higher interest in information from the higher frequencies, namely the k2\_d1, k3\_d1 and k2\_d2 are also those, which are able to perform significantly above chance when trained on full data and validated on high-passed data.
    This suggests consistency between the gradient visualization and the performance analysis.

\end{itemize}


\section{Shifting the predicted time-point}\label{sec:shifting-the-predicted-time-point}
In this section we describe how the performance and gradients change when the predicted time-point is shifted with respect to the receptive field.
Two kinds of analyses are introduced here.
\begin{itemize}
    \item Shifting the predicted time-point to the centre of the receptive field
This analysis was performed on all the network architectures and compares how the different architectures react to the shift both performance-wise and gradient-wise.
We highlight the differences and similarities between the architectures.
    \item Shifting the predicted time-point in steps across the receptive field
This analysis was performed only on the original Deep4Net sbp0.
It compares how the performance and gradients of the network change when the predicted time-point is shifted across the receptive field using a different ratio of information from the future and from the past.
\end{itemize}


\subsection{Shifting the predicted time-point to the centre of the receptive field}\label{subsec:shifting-the-predicted-time-point-to-the-centre-of-the-receptive-field}
An important observation when looking at the performance of the networks described in Section\ref{} was made.
We noticed that the smaller receptive field seemingly correlates with a higher prediction accuracy especially for absolute velocity decoding.
To visualize this, we created Figure~\ref{figure\_distance} which sorts the different architectures based on the size of the receptive field from smallest to largest and plots the average correlation coefficient each of these networks achieved.
There is a clear descending pattern.
The only exceptional network is the k2\_d3 network which performs well with a large receptive field.

This finding further corroborates the idea to shift the predicted time point to the center of the receptive field.
The receptive field as described in Section~\ref{methods\_receptive\_field} is non-uniform.
It considers mostly input-points in its centre while in the original decoding, the predicted time-point is located just outside the receptive field.
Therefore, we shift the inputs and prediction so that the iEEG signal, which was recorded at the same time as the predicted movement was executed, is in the centre of the receptive field. This causes the procedure to be unsuitable for online BCI because half of the input window uses information from the future.
The results can be seen in Figure~\ref{shifted\_performance}.
It is obvious that the shift greatly improves performance of all the networks and the performance differences between the architectures are diminished.
Notably the performance of the networks on the high-gamma dataset also increased especially for absolute velocity.

The improvement can be caused by two things:
\begin{itemize}
    \item By the network being able to focus on signals recorded directly before the movement execution.
    \item By the network having access to information from the future.
\end{itemize}

We hypothesise that is most likely a combination of the two.
Conclusion about how much each of the above described influences the prediction improvement cannot be made from the presented experiments.
It would be interesting to build a network with a uniform receptive field and then conduct experiments which would clarify this.
Nevertheless, such an analysis is out of the scope of this thesis.

\subsubsection{Performance}
In Section \ref{architecture_comparison_gradients} we already saw that the best performing networks are those which use lower frequencies.
Nevertheless, how the shift influences the gradients remained an open question.
Hypothetically, the information about the movement in high-gama could be informative only in the moments directly preceding the movement.
Therefore the networks are unable to use it because they focus on the signal recorded too far in the past.
To test this hypothesis, we visualized the gradients of the networks trained in shifted settings.

The comparison of the gradients between the networks in the original non-shifted setting and the shifted setting where the predicted time point was centered to the centre of the receptive field.
This was performed for all the architectures for all their intermediate layers.
The complete results can be found in the Appendix.
We performed this gradient analysis on the 1. full dataset and 2. high-passed dataset.

\begin{itemize}
    \item Gradients of networks which were trained and validated on full data are displayed in Figure~\ref{shifted-gradients-different-architecture}.
    We chose this graph to illustrate the pattern common to almost all layers.
    What we observe is that the networks across all architecture seem to refine their focus to more narrow frequency bands.
    Figure~\ref{} illustrates this behaviour.
    For example the vel\_k1 network which is the one without max-pools has high-gradient values in the original setting for frequencies up to 25Hz when looking at gradients for motor channels.
    In the shifted setting the band with high gradient values narrows to frequencies close to 0.
    The network vek\_k2\_d2 is in the non-shifted setting focused mostly on frequencies between 25 and 60 Hz. Nevertheless, with shifting the gradients for a lot of frequencies in this band decrease, and the network focuses on frequencies around 25Hz. For no network did the shift cause an increase in the use of information from the high-gamma frequency band.
    Rather it seems, that the shift allowed access to less noisy information in the clearly informative bands such as the alpha and beta bands, and the network did not have to compensate with information from other frequencies.
    \item On networks which were trained and validated on high-passed data can be found in Figure \ref{}.
    Again, we only chose to display gradients of one layer to illustrate a behaviour shared by all layers.
    What we observe in the gradients of the networks trained on high-passed datasets is different from what we observe on gradients of networks trained on full datasets, maybe even the opposite.
    The networks trained on shifted data exhibit interest in the same or a broader range of frequencies above the 60Hz cut-off frequency than the networks trained in the original, non-shifted setting.
    This and the increase in performance on high-passed datasets suggests that indeed the information from the future or directly preceding the movement contains more information about movement in the high-gamma band.
    But the fact that the networks trained on full data do not use high-gamma, and their performance increases significantly compared to the non-shifted setting points to the information in high-gamma being informative but redundant when having access to information from all frequencies.
\end{itemize}

\subsubsection{Gradients}
Besides the big shift from the edge of the receptive field to the centre we also studied what happens if we shift the predicted time-point in smaller steps across the receptive field of the network.
Again, we do not know if the network's performance changes because of the shift in the receptive field or because of having access to information from the future.
Nevertheless, we can observe the slow decrease in performance when increasing the distance of the predicted time-point to the receptive field centre, which is to be expected for both of the reasons.
Besides evaluating the performance in the shifted settings, we also visualized the gradients as is described in this section, we also performed gradient visualization.
This analysis is described in Section~\ref{shifted\_gradient\_visualization}.

This analysis was performed only for the original Deep4Net architecture.
The shifts were made ranging from  -1 s  to 1 s with respect to the receptive field centre which we denote as 0.
The step size was 0.1 s which is equivalent to 25 samples.

\subsection{Shifting the predicted time-point across receptive field}\label{subsec:shifting-the-predicted-time-point-across-receptive-field}
\subsubsection{Performance}
For the original Deep4Net architecture (k3\_d3\_sbp0), we also visualized the gradients for all the smaller shifts across the receptive field of the network.
The gradients are visualized in Figure~\ref{figure\_shifted\_gradients}.
In the graph for absolute velocity, we can observe the trend of broadening the frequency ranges with high gradient values when shifting the predicted time-point from the centre of the receptive field both to the left and to the right.
Interestingly, when plotting the same graph for velocity, we do not observe this behaviour so clearly, instead there is a periodicity in the positive and negative value of the gradients for the different values of the shift.
It is unclear why this periodicity occurs.
\subsubsection{Gradients}
When looking at the gradients of the networks trained on whitened data, we observe that the networks indeed use modulations in the high-gamma frequency bands for their predictions.
Interestingly instead of having uniform gradients across all frequencies, they inverted their focus from low frequencies to high-frequencies.
The only architectures for which this did not happen are the models vel\_k1 and absVel\_k1 which are the networks without a max-pool layer.
When looking at the performance graphs in Figure~\ref{} though, these are the networks for which the performance dropped least.
This again suggests that the using information from the high-gamma frequency is possible, but it does not seem to help achieve better correlation coefficients.

\section{Spectral whitening}\label{sec:spectral-whitening}

\subsection{Performance}\label{subsec:pw-performance}
How the networks react to datasets which were whitened as described in Section~\ref{methods-spectral-whitening} was one of our interests because when we look at the spectrum of the original signal, the amplitudes of frequencies decrease exponentially with increase in the frequency.
This is common in biological signals but could be a reason for the network to ignore high-frequencies when making predictions. Figure \ref{pw_performances_full_data} shows how the predictions changed compared to predictions on non-whitened signals when using the full dataset.
It is obvious that the correlation coefficient of all networks dropped significantly. When looking at Figure \ref{pw-performaces-hp-data} we see that in the case of high-passed datasets the decrease was lower, but even for the high-passed data, the performance did not increase.

\subsection{Gradients}\label{subsec:pw-gradients2}